[notes]
### 2
 はじめにこの論文を選んだ動機ですが３つありまして、１つ目はRDMAに関する知識をつけたかったという点です。抱えていたタスクにRDMAが少し絡むことがあったのでそれも後押しになっていました。２つ目は軽く流し見したときにTensorflowとかをターゲットにしている雰囲気で、もしかしたら弊社の機械学習関係のインフラとして良さそうな感じのものか興味が湧いたからです。３つ目は仮想化するのにベアメタル構成と同じくらいパフォーマンス出るよと書いてあって、そんな訳あるかと思ったからです。こんな理由から今回はこの論文を取り上げさせてもらいました。
### 4
 まず、本論文の概要をざっくりと説明します。近年、大規模クラウド環境でコンテナを利用したアプリケーション開発が盛んにされています。コンテナを利用するメリットは大きくて、高リソース効率であることや、軽量な隔離環境でアプリケーションが実行できることなどが挙げられると思います。ただし、その一方でデータインテンシブなアプリケーション、例えばディープラーニング関係のものや、データアナリティクス関係のものは、高性能なネットワーク環境、それこそRDMA環境を要求することが多々あります。この論文では、コンテナ化によるメリットを享受しながらRDMAによるハイパフォーマンスネットワークを利用する方式として新たにFreeFlowを提唱し、検証によりその性能を確認した、という話です。
### 5
 まず背景からお話します。先程話したとおり、コンテナ化には様々なメリットがあり現在ではアプリケーションマネジメントの方式としては非常に復旧しています。ここでクラウド環境におけるコンテナの利用を考えたときに、コンテナは以下の要件を満たすことで非常に効率的で柔軟なアプリケーション環境を提供することができると、筆者らは定義付けています。１つ目は、ContainerがそれぞれのNetwork namespaceを持っていること。（Isolation）２つ目は、virtual networkを利用し、どのホストでも固定のvirtual ipを利用してコンテナ間通信ができること。（Portability）３つ目は、コントロールプレーン、データプレーンのポリシーを柔軟にコントロールできること。です。\par これらの全てを満たした状態でRDMAの環境を提供することが目的になります。ただRDMAの完全仮想化を行えばこれらは達成できそうだけどパフォーマンスが出ないし、逆にRDMAをパススルーする方式をうまくやろうとしてもコントロールプレーンやデータプレーンのポリシー変更が柔軟ではなくなってしまうという問題がある。ちなみに現在だと、データインテンシブなアプリケーションは専用ベアメタル＋RDMAでパフォーマンスを担保するようなケースが多いようで、この形態は提供側もユーザ側もしんどいよね、という感じ。
### 6
 そのため本論文では、先に上げた全てのコンテナ化のメリット＋パフォーマンスがベアメタルくらい出るようなRDMA環境をコンテナに提供することが目的です。現時点で存在する、コンテナにRDMA通信を可能にさせる方式と、今回の評価軸にのっとって評価した場合の結果を表にしています。* SR-IOVに関しては本質的にPortabilityに難あり。* HyVなるものがあるらしい（これは知らないしちゃんと調べられていないが）Visibilityやdata trafficのcontrolができないらしい。* SoftRoCEというUDPネットワークスタックの上でRDMAを実行するソリューションは、既存のIPネットワークソリューションを利用して多くを実現できるものの、パフォーマンスはUDPで制限されてしまうため出ない。\par 本論文紹介する方式はFreeFlowと読んでいます。以降ではFreeFlowを概要から追って説明していきます。
### 7
 RDMAではAPIを利用してアプリケーションがHW NICに対してコマンドを投げるような形になっています。FreeFlowではこのアプリケーションとHW NICの間に一層作る形になります。図でいうとFreeFlow Routerと記述のあるグレーの部分です。このFreeFlow Routerでvirtual networkの提供と、コントロールプレーン、データプレーンのポリシーの適用を行います。またFreeFlow Routerでは、コンテナのメモリ領域をshareしており、アプリケーションからNICへのデータコピーも、NICからアプリケーションへのデータコピーも、FreeFlow Routerのメモリを橋渡しとする形で行います。
### 8
 全体のアーキテクチャの概要がこちらの図になります。これから各部分（FreeFlow Network Library, FreeFlow Router, FreeFlow orchestrator）について説明をしていきます。
### 9
 まずはFreeFlow Network Libraryです。これはアプリケーションが利用するRDMAのVerbs libraryを改良した感じのものになります。ただこれは後ほど説明しますが、Application側が利用するAPIにはほぼ手を付けない形で改良するため、通常のRDMA通信コードから、FreeFlowの通信にするためには、ライブラリの差し替えを行うだけで、アプリケーション側でコードを変更する必要がほとんどないようになっています。FFLは次に説明するFreeFlow Routerとやり取りを行います。
### 10
 FreeFlow Routerはホスト毎に一つコンテナで起動し、そのホストに対してvirtual networkの機能提供を行う。FFLとのIPC channelをコントロールすることでデータプレーンのリソースポリシーなどを実現します。次に説明するFFOを連携することでIPアドレスなどのタスク処理なども行うらしいです。FFRが提供するvirtual networkというのがRDMAのみの話ではないのかとも思ったのですが、ちょっと論文の中身では触れられていなかったです。ただ実際、Githubの実行例を見ると docker runのタイミングでweaveネットワーク指定できるみたいで、任意のコンテナオーバーレイネットワークソリューションを使用できそうでな形になっていて、TCP/IPで通信をすることもできるみたいなのでそのへんのやり取りのことかなと思っています。
### 11
 FFOはコントロールプレーンを司るようなコンポーネントらしいんですが紙面の都合上、FFOの詳細はあまり書いてなかったです。コンテナ間通信のコントロールやリアルタイムモニタリングなどを行うようです。FFOではZooKeeperを利用してユーザ定義の情報。例えばIPアサインの情報や、アクセスコントロール、リソース共有のポリシー、メモリに関する情報などを管理するそうです。
### 12
 アプリケーションがFFL側で実行したRDMAのAPIコールを、FFR側で実行する必要があります。というのも、アプリケーションがコールした内容を、FFR経由でそのホストのPhysical NICに同様のリクエストを必要があるからです。つまりあるコンテナから別のコンテナに対してAPI実行をバイパスする必要がある、ということをする。直感的に考えつく方法としてRPCがあるかと思います。今回は、次に説明する2種類の方法で実現します。１つ目は、File Descriptorをうまく変更する方法で、言ってしまえばUnix domain socketによるIPCになります。２つ目は、Fastpathという手法で、これはイメージとしてはShared memoryによるIPCになります。
### 13
 まずはFile Descriptorをうまく変更する方式についてです。Verbs Libraryはレイヤ構造になっていて、特にApplication側は結構複雑な構造体のポインタがたくさん出てくる感じになっているので、RPCで行おうとするとかなり深いDeepCopyなどをしないと行けなくて、速度面で問題になってしまいます。ただ、NICファイルディスクリプタに対して行われる要求部分は比較的単純になっているらしく、ここをUnix domain socketに差し替え、もう片方の端点をFreeFlow Routerにするようにすれば、FreeFlow Routerに対してNICファイルディスクリプタに対して実行される内容がFFRに伝わることになるので、FFRはそれと同じことをPhysical NICに対して行ってあげる、という手法になります。このように、うまくレイヤ構造になっているライブラリの低い部分を差し替えるため、アプリケーション側としては通常のAPIコールをするだけでよいようになっている。
### 14
 ただしこの方式はレイテンシがどうしても大きくなってしまうため、レイテンシセンシティブなアプリケーションではまだ微妙です。この方式の良い点はCPUをほとんど利用しない点なのですが、逆にこのボトルネックを解消するためにCPUリソースを少し利用する設計が、次に説明するFastpathになります。
### 15
 Fastpathは言ってしまえば、shared memoryを利用するIPCになります。まずFastpathでは、FFLとFFRでメモリをshareしておきます。FFR側ではCPUを（おそらく）pinningし、shared memoryの内容を確認します。FFL側ではリクエスト内容をメモリに書き、その内容を受け取った、FFRが直ちにそのリクエスト内容を処理します。このタイミングでFFL側でもCPUを利用して、レスポンスを待ち受けます。FFRから同様にレスポンス内容がメモリに書かれると、それがFFLに渡り、処理されます。FFL側では処理し終わったら、CPUを手放します。このようにshared memoryを利用してプロセス間通信をします。
### 16
 Fastpathのメリットはレイテンシが小さいことですが、デメリットはもちろんCPUを消費することです。実装的にできる限りCPUの利用を削減するために、いくつか制限を設けているようです。１つ目は、1つのFFR、つまり1ホストでは1CPUのみを利用するように、複数コンテナの着信確認を1CPUでおおなうようなデザインにしたということです。２つ目は、non-blockingな関数の場合にのみ利用するなどの制限を設け、FFL側でのCPU利用時間は極めて少なくしたということです。また、FFOが、ホスト上にレイテンシセンシティブなアプリケーションが存在しないことを確認できる場合はFastpathとCPUの利用を止めるようにできるそうです。その判定は、起動しているコンテナの内容で判断すると一言書いてありましたが、詳細はあまりわからなかったです。
### 17
 次にコントロールプレーン、データプレーンのポリシーに関する話です。これは論文的にも必要最低限しか書いてなく、「紙面の都合で割愛する」という言葉が結構出てきていました。FreeFlowでは基本的にコントロールプレーン／データプレーンの両方のオペレーションが可能で、一般的なポリシーの適用は可能であると記述がありました。挙げられていたものとしては、帯域幅の制御や、フローの順位づけ、リソースの制限などです。基本的にRDMAはメッセージ単位の通信になるため、そのレベルでの制限はつけられるというような肌感です。コントロールプレーンの例とデータプレーンの例を簡単ですが挙げてくれていました。
### 18
 まずコントロールプレーンのポリシーの例です。RDMAにおける通信の端点（TCPとかUDPでいうsocketみたいなもの）にQueue Pairというのがあるんですが、各コンテナが作成できるQPの数に制限をつけることができます。一般にRDMAではこのQPの数が膨大になると性能に影響することが知られているようで、これを各アプリケーションが起動するコンテナ単位でリミットを設けてハンドリングすることができます。\par (理論的にQPは一つのHCA（コントローラデバイス）ごとに2^24個作れるものの、メモリの制限で4万～7万個くらいがせいぜいらしい) 
### 19
 次にデータプレーンのポリシーの例です。フローごとにレートリミットをかけることができるようです。FFRの中に簡単なトークンバケットのデータ構造を実装していてまず、アプリケーションがQueue Pairを作成したときにFFOにポリシーチェックをし、予め設定されていたレート制限を持つトークンバケットをQueue Pairに関連付けて、アプリケーションがリクエスト送信要求をするたびに、FFRがトークンバケットを確認し送信するかどうかをハンドリングするという流れでレートリミットを実装する形になります。ただ、この実装はあくまで今回の論文用の一例らしく、実際にはもっと自由に実装できるようにAPIの提供などを行っているようです。\par このようにして、コントロールプレーンやデータプレーンにポリシーを適用することができます。
### 20
 評価に利用するサーバー構成としては2種類を整えた。1つ目はInfinibandで通信する構成のもの、2つ目はRoCEで通信する構成のものになります。
### 21
 RDMAのSEND/WRITEのlatency, bandwidthを測定しました。ベンチマーク測定用のツールはFreeFlow上でコードの変更なしに実行できたそうです。
### 22
 まずはスループットについて、先程述べた2種類の構成で測定したものを、それぞれベアメタル+RDMAの場合と比較した図がこちらの図になります。この図は、READの結果ですが、Writeの結果もほぼ同様になったそうで、論文には掲載されていませんでした。測定時は1GBのデータを2KBから1MBの単位で分割して転送しています。（横軸）メッセージサイズが8KByte以上の場合はInfiniband / RoCEともにベアメタルの性能と同じになっています。8KByte未満のところはサイズが細かすぎて帯域が飽和していないだけのようで、実際に2CPUをFFRに利用するとHost-RDMAと同じような値になったようです。
### 23
 同時稼働するコンテナペア（フロー数）を最大512まで増加させても、全てのフローの集約スループットはほぼ一定の性能を維持していること、及びこのときの帯域幅は各フロードで均等に分さんしていることも測定されました。FreeFlowでコンテナを増加させても問題ないことがわかります。
### 24
 次に、レイテンシについてです。こちらはInfinibandの構成で測定しており、64Byte, 256Byte, 1KByte, 4KByteのメッセージ送信のレイテンシを測定しました。こちらに関してはWRITEとSENDの両方の結果を掲載していました。WRITEのほうがSENDに比べレイテンシが少なっく、FreeFlowとベアメタルのギャップも小さいです。これは今回のスライドでは説明を省いていますが、RDMAの通信方式によりFFLとFFRの間で起こるIPCに違いがあり、その部分のレイテンシが影響しているためです。とはいっても、SENDの場合であってもHost-RDMAに比べ余剰のレイテンシは1.5$\mu $s程度です。
### 25
 ちなみに1.5$\mu $sってどれくらいかというのも丁寧に書いてくれていました。大体、ネットワークのワンホップにかかる時間が0.55$\mu $s程度なので、これが比較対象として上がってくるくらいの速度のようです。TCP stackは最低でも10$\mu $s程度、TCP virtual network latencyは40$\mu $s程度にもなるそうなので感覚的喉の程度の速度かはわかっていただけるかと思います。
### 26
 また、CPUオーバーヘッドに関しても測定しておく。Fastpathを用する場合とそうでない場合のレイテンシとCPU coreの利用率を見るとこちらのようになっています。まずレイテンシの観点から行くとHost-RDMAが1.8$\mu $sである一方、Fastpathだと2.4$\mu $s、LowCPU（これは名言はなかったですが、おそらくfdを利用する方式）では17.0$\mu $s程度になってしまいます。一方で、FastpathはCPUを1つフルに使用しますが、LowCPUでは全然利用しないことがわかります。
### 27
 rate limitに関しても検証を行い、データプレーンポリシーがうまく適用されているかの検証も行う。frow rateに制限をかけて、帯域幅を変更しながら検証したところ理想的な値を推移することがわかった。この実験を、複数コンテナペア間でリミットを変更しながら確認しても問題なく動くことがわかった。
### 28
 rsocket - RDMA socket APIのこと。socket-level APIとしてRDMAを提供してくれているプロトコルで、通常のsocket callと同様の取り回しでRDMAを利用できる。内部的にはsocketとverbsの変換処理を行っているらしく、Host-TCPほどの性能は出ないものの、Weaveよりは性能が出ている。\par rsocketを利用した性能の検証も行ったようです。iperfでTCPのスループット、NPtcpでTCPのレイテンシをそれぞれ計測したところ、virtual nnetworkであるWeaveと比較すると常に早く、Host-TCPには勝てないくらいの性能にはなった。Host-TCPに負けている理由はsocketとverbsの間の変換に関するoverheadが原因だということでした。
### 29
 ここからは現実的なアプリケーションを取り上げて性能検証の話をしていきます。今回はTensorflowとSparkを取り上げ、機械学習とデータ・アナリティクスのアプリケーションの性能を測定しました。Infinibandの構成で行い、比較対象はHost-RDMA、Host-TCP、Weaveの三種類です。
### 30
 まずはtensorflowの性能検証から話します。3台のサーバー上でRDMA対応のtensorflowを実行します。各サーバにはGPUが搭載されており、一基がmasterかつparameterサーバ、2基がworkerサーバーの構成です。training workloadとして、2種類、CNNとRNNを実行しています。
### 31
 まずはCNNの方の結果を話します。モデルとして利用したのは3種類で、トレーニングデータとしてはImageNetのデータを利用したとのことです。分散トレーニングをするわけですが、結果からどのモデルに関しても同じ傾向が見られるため、ネットワークパフォーマンスがボトルネックになっていることがわかります。Host-RDMAとHost-TCPを比較するとRDMAのほうが1.8～3倍程度早いです。また、FreeFlowとWeaveを比較すると更に大きい差がでています。そのようななか、FreeFlowはHost-RDMAに迫る性能が出ていることがわかります。
### 32
 次にRNNの方の結果です。RNNの設定部分の話はイマイチわかりませんでした。。。こちらは各訓練ステップに費やされた時間をCDFグラフにしたものです。CNNの場合と同様の傾向を示していて、FreeFlowはHost-RDMAに迫る性能が出ています。
### 33
 次にSparkの結果について話します。こちらは2つのサーバーでSparkを実行し、片方にはmaster, slaveのコンテナ, もう片方にはslaveのコンテナで動作させたそうです。Sparkに同梱されているGroupByとSortByというベンチマークで計測したようです。結果をみると、こちらもtensorflowと同様の結果になっていることがわかります。こちらでもFreeFlowはHost-RDMAに迫る性能が出ていることがわかります。
### 34
 最後にこの方式でいくつか議論になりそうな話に関してまとめて議論しておきます。まず、CPUオーバーヘッドについては今回はあえてレイテンシ改善のために許容しました。もしうまくハードウェアにオフロードできれば解決はできるかもねという言葉とともに、今後の課題としていました。\par 輻輳制御に関してはRDMA NIC側に輻輳制御メカニズムが入っており、FreeFlowはHW NICとしてそれを利用するため、そちらに準拠することになります。
### 35
 次にセキュリティー的な問題についてです。FFRでは各コンテナのメモリを共有するから、IPC領域スキャンすると別のコンテナのメモリ状態見れるのではという問題が考えられますが、FFRでは個々のQueue Pairに対して独立して共有メモリバッファを作成するため、スキャンしても他のコンテナ情報などを読み取ることはできなくなっています。\par また、これは私がわからなかったんですが、memory keyの問題があるよと書いてありました。ただこれに関してはRDMAの問題なのでFreeFlowでどうこうという話ではないよね、とのことでした。
### 36
 コンテナマイグレーションに関しては、まずオフラインマイグレーション、すなわちシャットダウンして、移動して、再起動する形のマイグレーションに関しては可能で、virtual IP が変わらないのでそのままRDMAの接続を再確立するそうです。（これはRoCEみたいなIP層を利用するものだとわかりますが、InfinibandだとそもそもIP使うっけ？みたいなレベルの理解が無いのでちょっとわかりません。）ただしライブマイグレーションはできないということでした。
### 37
 VM上でコンテナを起動させる場合にFreeFlowが使えるのかという問題に関しては、今回のプロトタイプでは基本的にコンテナはベアメタル上で動かしていたが、SR-IOVなどを利用しているVMであれば問題ないだろうと書いてありました。その他のRDMAピアとの通信はどうなるのかという問題は、基本的に通信対象がFRRを利用しているかどうかなどは関係しないため、RDMAで普通に通信することが可能になっているということでした。
### 38
 結論です。FreeFlowによりコンテナの環境に対して、Isolation, Portability, Controllability, Performanceを満たすRDMA環境を提供することができるようになりました。Controllabilityに関してはmicrobenchmarkでうまくフロー制御ができていることがわかりました。肝心のPerformanceに関してもベアメタルRDMAに匹敵するくらいの性能が出ることをいくつかの性能検証で確認できました。一応Githubで公開してくれていて、まだプロトタイプ段階とのことですが、ハードウェア要件を満たせば試して確認することもできると思います。issueを見ると、k8sへの対応なども勧めていて、結構頑張っていそうだなという印象でした。以上で、発表は終わりです。何か質問等あればいただければと思います。論文の内容を省いている部分もあるので、話していない論文の内容についても何かあれば質問いただければと思います。
